# Deployment (`tensorflow`)

## üìñ Introduction
Deploying TensorFlow models enables production use. This guide covers model export (SavedModel), serving (TensorFlow Serving), and edge deployment (TensorFlow Lite, TensorFlow.js), with practical examples and interview insights.

## üéØ Learning Objectives
- Export models as SavedModel for serving.
- Serve models with TensorFlow Serving.
- Deploy models on edge devices with TensorFlow Lite/JS.

## üîë Key Concepts
- **SavedModel**: Standard TensorFlow format for export.
- **TensorFlow Serving**: Scalable serving via REST/gRPC.
- **TensorFlow Lite**: Lightweight models for mobile/edge.
- **TensorFlow.js**: Browser-based inference.

## üìù Example Walkthrough
The `deployment.py` file demonstrates:
1. **Model**: Training a CNN on MNIST.
2. **SavedModel**: Exporting the model.
3. **TensorFlow Serving**: Instructions for serving.
4. **TensorFlow Lite/JS**: Converting and evaluating.
5. **Visualization**: Visualizing predictions.

Example code:
```python
import tensorflow as tf
model = tf.keras.Sequential([...])
model.save("saved_model/mnist_cnn")
```

## üõ†Ô∏è Practical Tasks
1. Export a trained model as SavedModel.
2. Set up TensorFlow Serving for the model.
3. Convert a model to TensorFlow Lite and evaluate it.
4. Prepare a model for TensorFlow.js deployment.
5. Visualize predictions from a deployed model.

## üí° Interview Tips
- **Common Questions**:
  - What is the SavedModel format?
  - How does TensorFlow Serving handle requests?
  - Why use TensorFlow Lite for edge devices?
- **Tips**:
  - Explain SavedModel‚Äôs portability.
  - Highlight TensorFlow Lite‚Äôs quantization benefits.
  - Be ready to describe a deployment pipeline.

## üìö Resources
- [TensorFlow SavedModel Guide](https://www.tensorflow.org/guide/saved_model)
- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)
- [TensorFlow Lite](https://www.tensorflow.org/lite)
- [TensorFlow.js](https://www.tensorflow.org/js)